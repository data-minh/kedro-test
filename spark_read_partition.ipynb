{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6151039f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/26 08:39:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession đã được tạo thành công.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit \n",
    "\n",
    "# 1. Tạo SparkSession\n",
    "# appName: Tên ứng dụng Spark của bạn\n",
    "# master: 'local[*]' nghĩa là Spark sẽ chạy cục bộ trên tất cả các nhân có sẵn\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSVtoParquet\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession đã được tạo thành công.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce4bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"/home/phamminh/work/BIDV/kedro_research/kedro-spark/data/spark_data/source/data.csv\"  # Đảm bảo file data.csv nằm trong cùng thư mục với script này\n",
    "df = spark.read.csv(csv_file_path, inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8bb3c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---------------+\n",
      "| id|   name|   age|processing_date|\n",
      "+---+-------+------+---------------+\n",
      "|  1|  Alice|    30|     2025-01-02|\n",
      "|  2|    Bob|    24|     2025-01-02|\n",
      "|  3|Charlie|    35|     2025-01-02|\n",
      "|  4|   Minh|    25|     2025-01-03|\n",
      "|  5|mmmmmmm| 22222|     2025-01-05|\n",
      "|  6| djashd| 22222|     2025-01-05|\n",
      "|  7|asjkdsa|238927|     2025-01-05|\n",
      "|  8|   dshd| 23532|     2025-01-07|\n",
      "|  9|   dshd| 23532|     2025-01-07|\n",
      "| 10|   dshd| 23532|     2025-01-07|\n",
      "| 11|   dshd| 23532|     2025-01-08|\n",
      "| 12|   dshd| 23532|     2025-01-08|\n",
      "| 13|   dshd| 23532|     2025-01-09|\n",
      "+---+-------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f2415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_date_string = \"2025-01-02\"\n",
    "# df1 = df.withColumn(\"processing_date\", lit(custom_date_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346965e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+---------------+\n",
      "| id|   name|age|processing_date|\n",
      "+---+-------+---+---------------+\n",
      "|  1|  Alice| 30|     2025-01-02|\n",
      "|  2|    Bob| 24|     2025-01-02|\n",
      "|  3|Charlie| 35|     2025-01-02|\n",
      "+---+-------+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_output_path = \"/home/phamminh/work/BIDV/kedro_research/kedro-spark/data/dynamic_data/reviews\"\n",
    "# partitionBy(\"processing_date\"): Spark sẽ tạo các thư mục con dựa trên giá trị của cột 'processing_date'\n",
    "df.write.mode(\"overwrite\").partitionBy(\"processing_date\").parquet(parquet_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b920f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(\"/home/phamminh/work/BIDV/kedro_research/kedro-spark/data/dynamic_data/reviews/2025-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6086be11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------------+-----------------------+------------------+------------------+----------------------+-------------------+-----------------+-----------------+\n",
      "|shuttle_id|review_scores_rating|review_scores_comfort|review_scores_amenities|review_scores_trip|review_scores_crew|review_scores_location|review_scores_price|number_of_reviews|reviews_per_month|\n",
      "+----------+--------------------+---------------------+-----------------------+------------------+------------------+----------------------+-------------------+-----------------+-----------------+\n",
      "|     45163|                91.0|                 10.0|                    9.0|               9.0|               9.0|                   9.0|                9.0|               26|             0.77|\n",
      "|     49438|                96.0|                 10.0|                   10.0|              10.0|              10.0|                  10.0|                9.0|               61|             0.62|\n",
      "|     10750|                97.0|                 10.0|                   10.0|              10.0|              10.0|                  10.0|               10.0|              467|             4.66|\n",
      "|      4146|                95.0|                 10.0|                   10.0|              10.0|              10.0|                   9.0|                9.0|              318|             3.22|\n",
      "|      5067|                97.0|                 10.0|                    9.0|              10.0|              10.0|                   9.0|               10.0|               22|             0.29|\n",
      "|     14891|                93.0|                  9.0|                    8.0|              10.0|               9.0|                   9.0|                9.0|               15|             0.19|\n",
      "|      5689|                92.0|                  9.0|                   10.0|               9.0|              10.0|                  10.0|                9.0|               75|             0.79|\n",
      "|     75321|                97.0|                 10.0|                   10.0|              10.0|              10.0|                   9.0|               10.0|              450|              4.5|\n",
      "|     27341|                NULL|                 NULL|                   NULL|              NULL|              NULL|                  NULL|               NULL|                0|             NULL|\n",
      "|     25733|                94.0|                 10.0|                    9.0|              10.0|              10.0|                   9.0|                9.0|              168|             2.55|\n",
      "|     38733|                88.0|                 10.0|                    8.0|               9.0|               9.0|                   9.0|                9.0|               38|             0.41|\n",
      "|     61198|                96.0|                  9.0|                    9.0|              10.0|              10.0|                  10.0|                9.0|               95|             0.95|\n",
      "|      9858|                NULL|                 NULL|                   NULL|              NULL|              NULL|                  NULL|               NULL|                0|             NULL|\n",
      "|     18221|                95.0|                  9.0|                   10.0|              10.0|              10.0|                  10.0|                9.0|              184|              1.9|\n",
      "|     18590|                95.0|                 10.0|                    9.0|              10.0|              10.0|                   9.0|               10.0|              172|             1.91|\n",
      "|     19813|                72.0|                  7.0|                    8.0|               8.0|               8.0|                   9.0|                8.0|               10|             0.15|\n",
      "|     74856|                95.0|                  9.0|                   10.0|              10.0|              10.0|                   9.0|                9.0|              176|             1.95|\n",
      "|     76722|                98.0|                 10.0|                   10.0|              10.0|              10.0|                  10.0|               10.0|               30|             0.41|\n",
      "|     29755|                93.0|                  9.0|                   10.0|              10.0|              10.0|                   9.0|                9.0|               23|             0.27|\n",
      "|     12358|                95.0|                  9.0|                    9.0|              10.0|              10.0|                  10.0|                9.0|               23|             0.28|\n",
      "+----------+--------------------+---------------------+-----------------------+------------------+------------------+----------------------+-------------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e698f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(parquet_output_path).filter(\"processing_date >= '2025-01-05' and processing_date <= '2025-01-08'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd0c70f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---------------+\n",
      "| id|   name|   age|processing_date|\n",
      "+---+-------+------+---------------+\n",
      "|  8|   dshd| 23532|     2025-01-07|\n",
      "|  9|   dshd| 23532|     2025-01-07|\n",
      "| 10|   dshd| 23532|     2025-01-07|\n",
      "| 11|   dshd| 23532|     2025-01-08|\n",
      "| 12|   dshd| 23532|     2025-01-08|\n",
      "|  5|mmmmmmm| 22222|     2025-01-05|\n",
      "|  6| djashd| 22222|     2025-01-05|\n",
      "|  7|asjkdsa|238927|     2025-01-05|\n",
      "+---+-------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87240ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bắt đầu kiểm tra dữ liệu tháng 1/2025 ---\n",
      "Tổng số ngày dự kiến trong tháng 1/2025: 31 ngày.\n",
      "Tổng số ngày thực tế tìm thấy trong dữ liệu: 6 ngày.\n",
      "\n",
      "!!! LỖI: Dữ liệu tháng 1/2025 THIẾU các ngày sau:\n",
      "- 2025-01-01\n",
      "- 2025-01-04\n",
      "- 2025-01-06\n",
      "- 2025-01-10\n",
      "- 2025-01-11\n",
      "- 2025-01-12\n",
      "- 2025-01-13\n",
      "- 2025-01-14\n",
      "- 2025-01-15\n",
      "- 2025-01-16\n",
      "- 2025-01-17\n",
      "- 2025-01-18\n",
      "- 2025-01-19\n",
      "- 2025-01-20\n",
      "- 2025-01-21\n",
      "- 2025-01-22\n",
      "- 2025-01-23\n",
      "- 2025-01-24\n",
      "- 2025-01-25\n",
      "- 2025-01-26\n",
      "- 2025-01-27\n",
      "- 2025-01-28\n",
      "- 2025-01-29\n",
      "- 2025-01-30\n",
      "- 2025-01-31\n",
      "Tổng số ngày thiếu: 25 ngày.\n",
      "\n",
      "--- Kết thúc kiểm tra dữ liệu tháng 1/2025 ---\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, to_date, lit\n",
    "from datetime import date, timedelta\n",
    "import calendar\n",
    "\n",
    "def check_monthly_data_completeness(spark: SparkSession, parquet_base_path: str, year: int, month: int):\n",
    "    \"\"\"\n",
    "    Kiểm tra sự đầy đủ của dữ liệu theo ngày trong một tháng cụ thể\n",
    "    dựa trên partition 'processing_date'.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): Đối tượng SparkSession.\n",
    "        parquet_base_path (str): Đường dẫn cơ sở đến thư mục Parquet chứa dữ liệu đã partition.\n",
    "                                 Ví dụ: \"/path/to/your/data\"\n",
    "        year (int): Năm cần kiểm tra (ví dụ: 2025).\n",
    "        month (int): Tháng cần kiểm tra (ví dụ: 4 cho tháng 4).\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"--- Bắt đầu kiểm tra dữ liệu tháng {month}/{year} ---\")\n",
    "\n",
    "    # 1. Xác định tất cả các ngày trong tháng\n",
    "    num_days = calendar.monthrange(year, month)[1] # Lấy số ngày trong tháng\n",
    "    expected_dates = []\n",
    "    for day in range(1, num_days + 1):\n",
    "        expected_date = date(year, month, day)\n",
    "        expected_dates.append(expected_date.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    print(f\"Tổng số ngày dự kiến trong tháng {month}/{year}: {len(expected_dates)} ngày.\")\n",
    "    # print(f\"Các ngày dự kiến: {expected_dates}\") # Có thể bỏ comment để kiểm tra\n",
    "\n",
    "    # 2. Đọc các processing_date hiện có từ dữ liệu\n",
    "    # Tạo đường dẫn cụ thể đến dữ liệu của tháng đó (nếu có thể để giảm tải)\n",
    "    # Tuy nhiên, nếu bạn chỉ muốn quét các partition, Spark sẽ tự động tối ưu.\n",
    "    # Đọc dữ liệu từ đường dẫn cơ sở và chọn cột processing_date\n",
    "    try:\n",
    "        # Sử dụng pattern để chỉ đọc các partition của tháng đó nếu có thể.\n",
    "        # Lưu ý: Spark sẽ tự động lọc partition khi đọc, nhưng việc chỉ định rõ ràng\n",
    "        #       có thể giúp đọc ít metadata hơn nếu có nhiều partition khác.\n",
    "        #       Tuy nhiên, cách phổ biến nhất là đọc toàn bộ bảng và lọc sau.\n",
    "        #       Với mục đích kiểm tra partition, chúng ta sẽ đọc từ base path\n",
    "        #       và sau đó trích xuất partition key.\n",
    "        df_existing = spark.read.parquet(parquet_base_path) \\\n",
    "                           .select(col(\"processing_date\")) \\\n",
    "                           .distinct() # Lấy các ngày duy nhất\n",
    "\n",
    "        # Chuyển đổi về định dạng \"YYYY-MM-DD\" để so sánh dễ dàng\n",
    "        existing_dates = [row.processing_date for row in df_existing.collect()]\n",
    "        existing_dates_str = [d.strftime(\"%Y-%m-%d\") for d in existing_dates] # Chuyển Date object sang string\n",
    "\n",
    "        print(f\"Tổng số ngày thực tế tìm thấy trong dữ liệu: {len(existing_dates_str)} ngày.\")\n",
    "        # print(f\"Các ngày thực tế: {existing_dates_str}\") # Có thể bỏ comment để kiểm tra\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"LỖI: Không thể đọc dữ liệu từ đường dẫn '{parquet_base_path}'. Vui lòng kiểm tra đường dẫn hoặc quyền truy cập. Chi tiết: {e}\")\n",
    "        return # Thoát hàm nếu không đọc được dữ liệu\n",
    "\n",
    "    # 3. So sánh và tìm các ngày thiếu\n",
    "    missing_dates = sorted(list(set(expected_dates) - set(existing_dates_str)))\n",
    "\n",
    "    # 4. Ghi log và báo cáo\n",
    "    if missing_dates:\n",
    "        print(f\"\\n!!! LỖI: Dữ liệu tháng {month}/{year} THIẾU các ngày sau:\")\n",
    "        for missing_date in missing_dates:\n",
    "            print(f\"- {missing_date}\")\n",
    "        print(f\"Tổng số ngày thiếu: {len(missing_dates)} ngày.\")\n",
    "        # Bạn có thể thêm logic để gửi email, kích hoạt cảnh báo, v.v. ở đây\n",
    "        # Ví dụ: raise Exception(f\"Dữ liệu tháng {month}/{year} thiếu {len(missing_dates)} ngày.\")\n",
    "    else:\n",
    "        print(f\"\\n--- TUYỆT VỜI! Dữ liệu tháng {month}/{year} ĐẦY ĐỦ tất cả các ngày. ---\")\n",
    "\n",
    "    print(f\"\\n--- Kết thúc kiểm tra dữ liệu tháng {month}/{year} ---\")\n",
    "\n",
    "\n",
    "# --- Cách sử dụng ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Khởi tạo SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DataCompletenessChecker\") \\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Đường dẫn đến thư mục Parquet gốc của bạn\n",
    "    # Ví dụ: Nếu dữ liệu của bạn ở /user/hive/warehouse/your_table/\n",
    "    # Và partition là /user/hive/warehouse/your_table/processing_date=2025-04-01/\n",
    "    # thì base_path là /user/hive/warehouse/your_table\n",
    "    parquet_data_path = \"data/spark_data/target/data_demo\" # THAY THẾ BẰNG ĐƯỜNG DẪN THỰC TẾ CỦA BẠN\n",
    "\n",
    "    # Năm và tháng bạn muốn kiểm tra\n",
    "    target_year = 2025\n",
    "    target_month = 1 # Tháng 4\n",
    "\n",
    "    check_monthly_data_completeness(spark, parquet_data_path, target_year, target_month)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
