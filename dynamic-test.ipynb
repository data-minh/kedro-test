{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17099c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:219: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:219: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/tmp/ipykernel_16380/3263342124.py:219: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  bucket_pattern = \"_\\d*\\.c\"\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/24 00:51:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/24 00:51:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as f\n",
    "from hdfs import InsecureClient\n",
    "from kedro.pipeline import node\n",
    "from pyspark.sql import DataFrame, SparkSession, Window\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyarrow as pa\n",
    "import pyarrow.fs\n",
    "from omegaconf import OmegaConf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"utils\").getOrCreate()\n",
    "params = OmegaConf.load(\"./conf/base/globals.yml\")\n",
    "\n",
    "def pandas_train_test_split(\n",
    "    df, test_size=None, train_size=None, random_state=None, shuffle=True,\n",
    "    stratify=None, subset_col_name=\"subset\", subset_name=[\"train\", \"test\"]\n",
    "):\n",
    "    if stratify:\n",
    "        stratify = df[stratify]\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df, test_size=test_size, train_size=train_size,\n",
    "        random_state=random_state, shuffle=shuffle, stratify=stratify\n",
    "    )\n",
    "\n",
    "    train[subset_col_name] = subset_name[0]\n",
    "    test[subset_col_name] = subset_name[1]\n",
    "    return pd.concat([train, test])\n",
    "\n",
    "def to_julian_str(date_str):\n",
    "    \"\"\"Convert YYYY-MM-DD to YYYYDDD format\"\"\"\n",
    "    return datetime.strptime(date_str, '%Y-%m-%d').strftime('%Y%j')\n",
    "\n",
    "def pipe(self, func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Functionality:\n",
    "    - Implementation of Pandas .pipe() function for better visibility\n",
    "    Applying chainable functions that expect Pyspark.DataFrame\n",
    "    --------------------------------------------------\n",
    "    Args:\n",
    "    - self: use self to add directly to the pyspark.DataFrame object\n",
    "    as part of attributes of the class.\n",
    "    - func: the function to be piped\n",
    "    - *args: any argument of the piped function\n",
    "    - *kwargs: any keywords argument of the piped function\n",
    "    --------------------------------------------------\n",
    "    Returns:\n",
    "    - func()\n",
    "    The return type of func\n",
    "    \"\"\"\n",
    "    return func(self, *args, **kwargs)\n",
    "\n",
    "\n",
    "DataFrame.pipe = pipe\n",
    "\n",
    "\n",
    "def rename_columns(df, rename_dict):\n",
    "    \"\"\"\n",
    "    Functionality:\n",
    "    - Renames selected columns in pyspark.DataFrame based on _prefix or _suffix\n",
    "    --------------------------------------------------\n",
    "    Args:\n",
    "    - df: pyspark.DataFrame containings columns to rename.\n",
    "    - rename_dict: dictionary specified what columns to rename and how\n",
    "    with prefix_ and _suffix.\n",
    "     Ex: {\n",
    "     'prefix_' : {'ftr' : ['age','gender']},\n",
    "      '_suffix': {'ratio' : ['avg_spending_3m','max_spending_3m']}\n",
    "     }\n",
    "    --------------------------------------------------\n",
    "    Returns:\n",
    "    - pyspark.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    assert (\n",
    "        \"prefix_\" and \"_suffix\"\n",
    "    ) in rename_dict, (\n",
    "        \"None of the eligible keywords ('prefix_','_suffix') is in dictionary\"\n",
    "    )\n",
    "\n",
    "    # Create a list to hold all column renaming expr\n",
    "    rename_expr = []\n",
    "\n",
    "    prefix_ = rename_dict[\"prefix_\"]\n",
    "    _suffix = rename_dict[\"_suffix\"]\n",
    "\n",
    "    # Done list\n",
    "    done_list = []\n",
    "\n",
    "    # Iterate over the dictionary\n",
    "    if isinstance(prefix_, dict):\n",
    "        for prefix, columns in prefix_.items():\n",
    "            for col_name in columns:\n",
    "                rename_expr.append(\n",
    "                    f.col(col_name)\n",
    "                    .alias(f\"{prefix}_{col_name}\")\n",
    "                    )\n",
    "                done_list.append(col_name)\n",
    "\n",
    "    if isinstance(_suffix, dict):\n",
    "        for suffix, columns in _suffix.items():\n",
    "            for col_name in columns:\n",
    "                rename_expr.append(\n",
    "                    f.col(col_name)\n",
    "                    .alias(f\"{col_name}_{suffix}\")\n",
    "                    )\n",
    "                done_list.append(col_name)\n",
    "\n",
    "    # Add the rest of the columns that are not renamed (if any)\n",
    "    remaining_col_expr = [f.col(c) for c in df.columns if c not in done_list]\n",
    "\n",
    "    return df.select(*rename_expr, *remaining_col_expr)\n",
    "\n",
    "\n",
    "def backfill(spk_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Functionality:\n",
    "    - Backfill limit by the pervious limit\n",
    "    --------------------------------------------------\n",
    "    Args:\n",
    "    - spk_df: DataFrame containing limit\n",
    "    --------------------------------------------------\n",
    "    Returns:\n",
    "    - pyspark.DataFrame\n",
    "    \"\"\"\n",
    "    return spk_df.withColumn(\n",
    "        \"current_limit\",\n",
    "        f.first(f.col(\"current_limit\"), ignorenulls=True).over(\n",
    "            Window\n",
    "            .partitionBy(\"card_nbr\")\n",
    "            .orderBy(\"date\")\n",
    "            .rowsBetween(0, sys.maxsize)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_folder_date(df):\n",
    "    date = (\n",
    "        df\n",
    "            .withColumn(\n",
    "                \"date\",\n",
    "                f.regexp_extract(\n",
    "                    f.input_file_name(),\n",
    "                    \"\\\\d{4}-\\\\d{2}-\\\\d{2}\", 0\n",
    "                )\n",
    "            )\n",
    "            .first()\n",
    "            [\"date\"]\n",
    "    )\n",
    "    return date\n",
    "\n",
    "def read_bucket(\n",
    "    spark,\n",
    "    num_buckets,\n",
    "    bucket_cols,\n",
    "    path,\n",
    "    schema=None,\n",
    "    check_num_buckets=True\n",
    "):\n",
    "    \"\"\"\n",
    "    A helper function to read bucket if the system doesn't\n",
    "    implement Metadata Catalog\n",
    "    Author: ThanhNM3\n",
    "    \"\"\"\n",
    "    table_name = f\"table_{random.randint(0, sys.maxsize)}\"\n",
    "    FileSystem = \\\n",
    "        spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "    URI = spark.sparkContext._gateway.jvm.java.net.URI\n",
    "    Configuration = \\\n",
    "        spark.sparkContext._gateway.jvm.org.apache.hadoop.conf.Configuration\n",
    "    Path = spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "\n",
    "    def get_file_paths(path):\n",
    "        fs = FileSystem.get(\n",
    "            URI(re.sub(\"[?|^|\\\\[|\\\\]|\\\\{|\\\\}]\", \"\", path)), Configuration()\n",
    "        )\n",
    "        file_status = fs.globStatus(Path(path))\n",
    "\n",
    "        deepest_folder = (map(\n",
    "            lambda x: x.getPath(),\n",
    "            file_status if any(x.isDirectory() for x in file_status) else\n",
    "                fs.globStatus(Path(\"/\".join(path.split(\"/\")[:-1])))\n",
    "        ))\n",
    "\n",
    "        folder = next((\n",
    "            x for x in deepest_folder if\n",
    "            len(list(\n",
    "                filter(\n",
    "                    lambda x: not x.getName().startswith(\"_\"),\n",
    "                    map(lambda x: x.getPath(), fs.listStatus(x))\n",
    "                )\n",
    "            )) > 0\n",
    "        ), None)\n",
    "\n",
    "        file_paths = list(map(\n",
    "            lambda x: x.toString(),\n",
    "            filter(\n",
    "                lambda x: not x.getName().startswith(\"_\"),\n",
    "                map(lambda x: x.getPath(), fs.listStatus(folder))\n",
    "            )\n",
    "        ))\n",
    "\n",
    "        return file_paths\n",
    "\n",
    "    def get_schema(df):\n",
    "        return \", \".join(map(lambda x: f\"{x[0]} {x[1]}\", df.dtypes))\n",
    "\n",
    "    if schema is None or check_num_buckets:\n",
    "        file_paths = get_file_paths(path)\n",
    "        if check_num_buckets:\n",
    "            bucket_pattern = \"_\\d*\\.c\"\n",
    "            num_discovered_buckets = len(set(\n",
    "                match.group(0) for match in\n",
    "                map(lambda x: re.search(bucket_pattern, x), file_paths)\n",
    "                if match\n",
    "            ))\n",
    "            assert num_discovered_buckets == num_buckets, \\\n",
    "                f\"Number of specified buckets [{num_buckets}] doesn't match\" \\\n",
    "                \"with number of discovered buckets [{num_discovered_buckets}].\"\n",
    "        if schema is None:\n",
    "            schema_ddl = get_schema(spark.read.parquet(file_paths[0]))\n",
    "        else:\n",
    "            schema_ddl = schema\n",
    "    else:\n",
    "        schema_ddl = schema\n",
    "\n",
    "    bucket_cols = \", \".join(bucket_cols)\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE {table_name} ( {schema_ddl} )\n",
    "        USING PARQUET\n",
    "        CLUSTERED BY ( {bucket_cols} )\n",
    "        SORTED BY ( {bucket_cols} ) INTO {num_buckets} BUCKETS\n",
    "        LOCATION \"{path}\"\n",
    "    \"\"\")\n",
    "    return spark.table(table_name)\n",
    "\n",
    "\n",
    "def write_bucket(\n",
    "    spark,\n",
    "    df,\n",
    "    num_buckets,\n",
    "    bucket_cols,\n",
    "    path,\n",
    "    repartition_before_write=True,\n",
    "    mode=\"overwrite\"\n",
    "):\n",
    "    table_name = f\"table_{random.randint(0, sys.maxsize)}\"\n",
    "    FileSystem = spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "    URI = spark.sparkContext._gateway.jvm.java.net.URI\n",
    "    Configuration = spark.sparkContext._gateway.jvm.org.apache.hadoop.conf.Configuration\n",
    "    Path = spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "\n",
    "    if mode == \"ignore\":\n",
    "        path_SUCCESS = path + \"/_SUCCESS\"\n",
    "        fs = FileSystem.get(URI(path_SUCCESS), Configuration())\n",
    "        if fs.exists(Path(path_SUCCESS)):\n",
    "            logger.warning(f\"Path: {path_SUCCESS} already exists. Skip.\")\n",
    "            return\n",
    "\n",
    "    df = (\n",
    "        df.repartition(num_buckets, bucket_cols)\n",
    "        if repartition_before_write else df\n",
    "    )\n",
    "    (\n",
    "        df\n",
    "            .write\n",
    "            .format(\"parquet\")\n",
    "            .bucketBy(num_buckets, bucket_cols)\n",
    "            .sortBy(bucket_cols)\n",
    "            .option(\"path\", path)\n",
    "            .saveAsTable(table_name)\n",
    "    )\n",
    "\n",
    "def correct_path(dataset):\n",
    "    fs_prefix = str(dataset._fs_prefix) if hasattr(dataset, \"_fs_prefix\") else \"\"\n",
    "    file_path = str(dataset._filepath)\n",
    "    url = \"\"\n",
    "    path = \"\"\n",
    "    if fs_prefix == \"\":\n",
    "        path = file_path\n",
    "    else:\n",
    "        url = fs_prefix + file_path.split(\"/\")[0]\n",
    "        path = \"/\" + \"/\".join(file_path.split(\"/\")[1:])\n",
    "    return url, path\n",
    "\n",
    "def get_flat_paths(path):\n",
    "    \"\"\"\n",
    "    Expand path patterns with wildcards into all matching paths and also return month-end dates.\n",
    "    \n",
    "    Args:\n",
    "        path: Path with pattern like /tmp/{2024-12-??}\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (all_paths, month_end_paths) where month_end_paths contains only month-end dates\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import calendar\n",
    "    from datetime import datetime\n",
    "    \n",
    "    full_month_paths = []\n",
    "    end_month_paths = []\n",
    "    \n",
    "    # Handle path with no pattern\n",
    "    if \"{\" not in path:\n",
    "        return [path],[path]\n",
    "    \n",
    "    # Extract base path and pattern part\n",
    "    parent_path = path.split(\"{\")[0]\n",
    "    remainder = path.split(\"}\")[-1] if \"}\" in path else \"\"\n",
    "    match = re.findall(r'\\{(.*?)\\}', path)\n",
    "    \n",
    "    if not match:\n",
    "        return [path], end_month_paths\n",
    "    \n",
    "    date_patterns = match[0].split(',')\n",
    "    \n",
    "    for pattern in date_patterns:\n",
    "        if \"??\" in pattern:\n",
    "            # Handle patterns like 2024-12-??\n",
    "            year_month_match = re.match(r'(\\d{4})-(\\d{2})-\\?\\?', pattern)\n",
    "            if year_month_match:\n",
    "                year, month = year_month_match.groups()\n",
    "                year_int, month_int = int(year), int(month)\n",
    "                \n",
    "                # Get number of days in the month\n",
    "                num_days = calendar.monthrange(year_int, month_int)[1]\n",
    "                \n",
    "                # Generate path for each day\n",
    "                for day in range(1, num_days + 1):\n",
    "                    date_str = f\"{year}-{month}-{day:02d}\"\n",
    "                    full_path = parent_path + date_str + remainder\n",
    "                    full_month_paths.append(full_path)\n",
    "                    \n",
    "                    # Check if it's the last day of the month\n",
    "                    if day == num_days:\n",
    "                        end_month_paths.append(full_path)\n",
    "            else:\n",
    "                # If not a recognized pattern, keep as is\n",
    "                full_path = parent_path + pattern + remainder\n",
    "                full_month_paths.append(full_path)\n",
    "        else:\n",
    "            # No wildcard, handle exact date\n",
    "            full_path = parent_path + pattern + remainder\n",
    "            full_month_paths.append(full_path)\n",
    "            \n",
    "            # Check if this is a month-end date\n",
    "            date_match = re.match(r'(\\d{4})-(\\d{2})-(\\d{2})', pattern)\n",
    "            if date_match:\n",
    "                year, month, day = map(int, date_match.groups())\n",
    "                last_day = calendar.monthrange(year, month)[1]\n",
    "                if int(day) == last_day:\n",
    "                    end_month_paths.append(full_path)\n",
    "    return full_month_paths, end_month_paths\n",
    "\n",
    "def check_hdfs_exist(url, path):\n",
    "    if url == \"\":\n",
    "        url = \"hdfs://hadoop-ttqtdl:8020\"\n",
    "    fs = pa.fs.HadoopFileSystem.from_uri(url)\n",
    "    fileSelector = pa.fs.FileSelector(path, allow_not_found=True)\n",
    "    try:\n",
    "        path_info = fs.get_file_info(fileSelector)\n",
    "    except:\n",
    "        return False\n",
    "    if len(path_info) == 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def generate_lookback_pattern(date, period, lookback):\n",
    "    \"\"\"\n",
    "    Generate a pattern string with lookback periods in the format {YYYY-MM-DD,YYYY-MM-DD,...}.\n",
    "    \n",
    "    Args:\n",
    "        date: Reference date in 'YYYY-MM-DD' format\n",
    "        period: Data period ('latest', 'daily', 'monthly', 'quarterly', 'yearly')\n",
    "        lookback: Number of periods to look back\n",
    "    \n",
    "    Returns:\n",
    "        String with pattern in format {period1,period2,...}\n",
    "    \"\"\"\n",
    "    import datetime as dt\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    \n",
    "    d = dt.datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    date_range = []\n",
    "    \n",
    "    for i in range(lookback):\n",
    "        if period == 'latest':\n",
    "            # Just use the reference date\n",
    "            date_str = (d - relativedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "        elif period == 'daily':\n",
    "            # Look back by days\n",
    "            date_str = (d - relativedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "        elif period == 'monthly':\n",
    "            # Look back by months with wildcard for days\n",
    "            date_str = (d - relativedelta(months=i)).strftime(\"%Y-%m-??\")\n",
    "            \n",
    "        elif period == 'quarterly':\n",
    "            # Look back by quarters with wildcard for month and day\n",
    "            quarter_date = d - relativedelta(months=i*3)\n",
    "            quarter = ((quarter_date.month - 1) // 3) + 1\n",
    "            date_str = f\"{quarter_date.year}-Q{quarter}\"\n",
    "            \n",
    "        elif period == 'yearly':\n",
    "            # Look back by years with wildcard\n",
    "            date_str = (d - relativedelta(years=i)).strftime(\"%Y-??-??\")\n",
    "            \n",
    "        else:\n",
    "            # Default to monthly if period is not recognized\n",
    "            date_str = (d - relativedelta(months=i)).strftime(\"%Y-%m-??\")\n",
    "            \n",
    "        date_range.append(date_str)\n",
    "    \n",
    "    return \"{\" + \",\".join(date_range) + \"}\"\n",
    "\n",
    "def create_iter_pipeline(\n",
    "    func,\n",
    "    inputs,\n",
    "    outputs,\n",
    "    period = 'latest',  # 'latest', 'daily', 'monthly', 'yearly', 'quarterly',\n",
    "    lookback=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create pipeline by looping through each period in the time range\n",
    "    and preprocess the data accordingly.\n",
    "    \n",
    "    Args:\n",
    "        catalog: Data catalog\n",
    "        period: Data period ('latest', 'daily', 'monthly', 'yearly', 'quarterly')\n",
    "        func: Processing function\n",
    "        inputs: Input dataset names (string or list)\n",
    "        outputs: Output dataset names (string or list)\n",
    "    \"\"\"\n",
    "    logger.info(\"=\"*20)\n",
    "    logger.info(f\"Check function {func.__name__} with period = {period} and lookback month = {lookback}\")\n",
    "    pipeline = []\n",
    "    if isinstance(inputs, str):\n",
    "        inputs = [inputs]\n",
    "    if isinstance(outputs, str):\n",
    "        outputs = [outputs]\n",
    "    \n",
    "    date_list = []\n",
    "\n",
    "    end_date = pd.Timestamp(datetime.strptime(params[\"END_DATE\"], '%Y-%m-%d'))\n",
    "    start_date = pd.Timestamp(datetime.strptime(params[\"START_DATE\"], '%Y-%m-%d'))\n",
    "    \n",
    "    if period == 'latest':\n",
    "        date_list = [end_date]\n",
    "    \n",
    "    elif period == 'daily':\n",
    "        date_list = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    elif period == 'monthly':\n",
    "        date_list = pd.date_range(start=start_date, end=end_date, freq='M')\n",
    "    \n",
    "    elif period == 'quarterly':\n",
    "        date_list = pd.date_range(start=start_date, end=end_date, freq='Q')\n",
    "\n",
    "    elif period == 'yearly':\n",
    "        date_list = pd.date_range(start=start_date, end=end_date, freq='Y')\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid period: {period}. Must be one of 'latest', 'daily', 'monthly', 'quarterly', 'yearly'\")\n",
    "    for date in date_list:\n",
    "        formatted_date = date.strftime('%Y-%m-%d')\n",
    "        pattern = generate_lookback_pattern(formatted_date, period, lookback)\n",
    "        formatted_inputs =  [x.format(pattern) if \"{\" in x else x for x in inputs]\n",
    "        formatted_outputs =  [x.format(pattern) if \"{\" in x else x for x in outputs]\n",
    "        formatted_name = f\"{func.__name__}.{formatted_date}\"\n",
    "        \n",
    "        formatted_inputs.append(f\"params:{formatted_date}\")\n",
    "        \n",
    "        all_input_exist = True\n",
    "        # missing_input = []\n",
    "        # for x in formatted_inputs:\n",
    "        #     if \"params\" in x:\n",
    "        #         continue        \n",
    "\n",
    "        #     url, path = correct_path(catalog._get_dataset(x))\n",
    "            \n",
    "        #     # Glob Path Processing\n",
    "        #     if \"{\" in path:\n",
    "        #         full_month_input_exist = True\n",
    "        #         end_month_input_exist = True\n",
    "        #         full_month_missing_input = []\n",
    "        #         end_month_missing_input = []\n",
    "        #         full_month_paths, end_month_paths = get_flat_paths(path)\n",
    "                \n",
    "        #         for flat_path in full_month_paths:\n",
    "        #             if not check_hdfs_exist(url, flat_path):\n",
    "        #                 full_month_input_exist = False\n",
    "        #                 full_month_missing_input.append(flat_path)\n",
    "\n",
    "        #         for flat_path in end_month_paths:\n",
    "        #             if not check_hdfs_exist(url, flat_path):\n",
    "        #                 end_month_input_exist = False\n",
    "        #                 end_month_missing_input.append(flat_path)\n",
    "\n",
    "        #         if (full_month_input_exist == False) & (end_month_input_exist == False):\n",
    "        #             all_input_exist = False\n",
    "        #             missing_input.append(full_month_missing_input)\n",
    "\n",
    "        #     else:\n",
    "        #         if not check_hdfs_exist(url, path):\n",
    "        #             all_input_exist = False\n",
    "        #             missing_input.append(path)\n",
    "        # output_not_exist = True\n",
    "        # for x in formatted_outputs: \n",
    "        #     url, path = correct_path(catalog._get_dataset(x))\n",
    "        #     mode = catalog._get_dataset(x)._save_args.get(\"mode\", \"overwrite\")\n",
    "        #     if mode == 'ignore':\n",
    "        #         FileSystem = spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "        #         URI = spark.sparkContext._gateway.jvm.java.net.URI\n",
    "        #         Configuration = spark.sparkContext._gateway.jvm.org.apache.hadoop.conf.Configuration\n",
    "        #         Path = spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "        #         path_SUCCESS = url + path + \"/_SUCCESS\"\n",
    "        #         fs = FileSystem.get(URI(path_SUCCESS), Configuration())\n",
    "        #         if fs.exists(Path(path_SUCCESS)):\n",
    "        #             logger.warning(\n",
    "        #                 f\"Existed output path {path_SUCCESS}. Skip node {formatted_name}.\"\n",
    "        #                 )\n",
    "                    # output_not_exist = False\n",
    "        output_not_exist = True\n",
    "\n",
    "        if all_input_exist & output_not_exist:\n",
    "            n = node(\n",
    "                func=func,\n",
    "                inputs=formatted_inputs if len(formatted_inputs) > 1 else formatted_inputs[0],\n",
    "                outputs=formatted_outputs if len(formatted_outputs) > 1 else formatted_outputs[0],\n",
    "                name=formatted_name\n",
    "            )\n",
    "            pipeline.append(n)\n",
    "        else:\n",
    "            logger.warning(\n",
    "                f\"Missing input path Skip node {formatted_name}.\"\n",
    "            )\n",
    "\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bafaf7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(df: DataFrame, date_str: str):\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7797ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/24/25 00:51:14] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Using <span style=\"color: #008000; text-decoration-color: #008000\">'conf/logging.yml'</span> as logging configuration. You can change this <a href=\"file:///home/phamminh/work/BIDV/kedro_research/kedro-spark/.venv/lib/python3.12/site-packages/kedro/framework/project/__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/phamminh/work/BIDV/kedro_research/kedro-spark/.venv/lib/python3.12/site-packages/kedro/framework/project/__init__.py#272\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">272</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         by setting the KEDRO_LOGGING_CONFIG environment variable accordingly.  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/24/25 00:51:14]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using \u001b[32m'conf/logging.yml'\u001b[0m as logging configuration. You can change this \u001b]8;id=936953;file:///home/phamminh/work/BIDV/kedro_research/kedro-spark/.venv/lib/python3.12/site-packages/kedro/framework/project/__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=761751;file:///home/phamminh/work/BIDV/kedro_research/kedro-spark/.venv/lib/python3.12/site-packages/kedro/framework/project/__init__.py#272\u001b\\\u001b[2m272\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         by setting the KEDRO_LOGGING_CONFIG environment variable accordingly.  \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from kedro.framework.session import KedroSession\n",
    "from kedro.pipeline import Pipeline, node, pipeline\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import os\n",
    "from kedro.framework.project import configure_project\n",
    "\n",
    "# os.environ[\"KEDRO_PACKAGE_NAME\"] = \"kedro_spark\"\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# params = OmegaConf.load(\"./conf/base/globals.yml\")\n",
    "# session =  KedroSession.create(project_path=Path.cwd())\n",
    "# context = session.load_context()\n",
    "# catalog = context.catalog    \n",
    "\n",
    "pipelines = []\n",
    "pipelines += create_iter_pipeline(\n",
    "    period=\"daily\",\n",
    "    inputs=[\n",
    "        \"01_raw.{}.foo_raw_df\", \n",
    "        ],\n",
    "    outputs=\"02_intermediate.{}.foo_intermediate_df\",\n",
    "    func=foo,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053f420e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mNode\u001b[0m\u001b[1m(\u001b[0mfoo, \u001b[1m[\u001b[0m\u001b[32m'01_raw.\u001b[0m\u001b[32m{\u001b[0m\u001b[32m2025-01-01\u001b[0m\u001b[32m}\u001b[0m\u001b[32m.foo_raw_df'\u001b[0m, \u001b[32m'params:2025-01-01'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'02_intermediate.\u001b[0m\u001b[32m{\u001b[0m\u001b[32m2025-01-01\u001b[0m\u001b[32m}\u001b[0m\u001b[32m.foo_intermediate_df'\u001b[0m, \u001b[32m'foo.2025-01-01'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mNode\u001b[0m\u001b[1m(\u001b[0mfoo, \u001b[1m[\u001b[0m\u001b[32m'01_raw.\u001b[0m\u001b[32m{\u001b[0m\u001b[32m2025-01-02\u001b[0m\u001b[32m}\u001b[0m\u001b[32m.foo_raw_df'\u001b[0m, \u001b[32m'params:2025-01-02'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'02_intermediate.\u001b[0m\u001b[32m{\u001b[0m\u001b[32m2025-01-02\u001b[0m\u001b[32m}\u001b[0m\u001b[32m.foo_intermediate_df'\u001b[0m, \u001b[32m'foo.2025-01-02'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
