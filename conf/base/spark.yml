# You can define spark specific configuration here.

spark.driver.maxResultSize: 3g
# spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
# spark.sql.execution.arrow.pyspark.enabled: true

# # https://docs.kedro.org/en/stable/integrations/pyspark_integration.html#tips-for-maximising-concurrency-using-threadrunner
spark.scheduler.mode: FAIR

# #add
# # spark.master: spark://192.168.1.220:7077
# # spark.submit.deployMode: client # client or cluster

# # read and write in s3
# spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
# spark.hadoop.fs.s3a.access.key: admin
# spark.hadoop.fs.s3a.secret.key: password
# spark.hadoop.fs.s3a.endpoint: http://172.17.0.1:9000
# spark.hadoop.fs.s3a.connection.ssl.enabled: false
# spark.hadoop.fs.s3a.path.style.access: true

# # # # spark.executor.memory: 512m
# # # spark.jars.packages: org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262

# # # Iceberg

# spark.jars.packages: org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.2,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.1,org.apache.iceberg:iceberg-aws-bundle:1.8.1,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262
# # spark.jars.packages: >
# #   org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.2,
# #   org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.1,
# #   org.apache.iceberg:iceberg-aws-bundle:1.8.1,
# #   org.apache.hadoop:hadoop-aws:3.3.4,
# #   com.amazonaws:aws-java-sdk-bundle:1.12.262

# spark.sql.extensions: >
#   org.projectnessie.spark.extensions.NessieSparkSessionExtensions,
#   org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

# spark.sql.catalog.nessie: org.apache.iceberg.spark.SparkCatalog
# spark.sql.catalog.nessie.type: rest
# spark.sql.catalog.nessie.uri: http://172.17.0.1:19120/iceberg
# spark.sql.catalogImplementation: in-memory
# spark.sql.catalog.nessie.scope: catalog sign


